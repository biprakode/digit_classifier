{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5196,"sourceType":"datasetVersion","datasetId":3147}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cupy as cp\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:31:21.789297Z","iopub.execute_input":"2025-10-03T15:31:21.789489Z","iopub.status.idle":"2025-10-03T15:31:26.620651Z","shell.execute_reply.started":"2025-10-03T15:31:21.789471Z","shell.execute_reply":"2025-10-03T15:31:26.619665Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mnist-digit-recognizer/train.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Basic EDA","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/mnist-digit-recognizer/train.csv\")\ndata.shape\ndata.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:31:26.622691Z","iopub.execute_input":"2025-10-03T15:31:26.623142Z","iopub.status.idle":"2025-10-03T15:31:29.970499Z","shell.execute_reply.started":"2025-10-03T15:31:26.623101Z","shell.execute_reply":"2025-10-03T15:31:29.969813Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n5      0       0       0       0       0       0       0       0       0   \n6      7       0       0       0       0       0       0       0       0   \n7      3       0       0       0       0       0       0       0       0   \n8      5       0       0       0       0       0       0       0       0   \n9      3       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n5       0  ...         0         0         0         0         0         0   \n6       0  ...         0         0         0         0         0         0   \n7       0  ...         0         0         0         0         0         0   \n8       0  ...         0         0         0         0         0         0   \n9       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n5         0         0         0         0  \n6         0         0         0         0  \n7         0         0         0         0  \n8         0         0         0         0  \n9         0         0         0         0  \n\n[10 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 785 columns</p>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# plotting distribution of images\nsns.countplot(x=\"label\" , data = data , palette='viridis')\nplt.title(\"Distribution of Digits in MNIST\")\nplt.xlabel(\"Digit Label\")\nplt.ylabel(\"Count\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:31:29.971245Z","iopub.execute_input":"2025-10-03T15:31:29.971467Z","iopub.status.idle":"2025-10-03T15:31:30.873451Z","shell.execute_reply.started":"2025-10-03T15:31:29.971448Z","shell.execute_reply":"2025-10-03T15:31:30.872692Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = data.drop('label' , axis=1).values / 255.0\ny = data['label'].values\ny = y.astype(np.float64)\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:31:30.874417Z","iopub.execute_input":"2025-10-03T15:31:30.874913Z","iopub.status.idle":"2025-10-03T15:31:31.347372Z","shell.execute_reply.started":"2025-10-03T15:31:30.874884Z","shell.execute_reply":"2025-10-03T15:31:31.346692Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n#X_cp = cp.array(X)\n#y_cp = cp.array(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:31:31.348241Z","iopub.execute_input":"2025-10-03T15:31:31.348548Z","iopub.status.idle":"2025-10-03T15:31:31.749672Z","shell.execute_reply.started":"2025-10-03T15:31:31.348517Z","shell.execute_reply":"2025-10-03T15:31:31.748757Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### imagifying data for CNN models","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport random\nimages = {}\n\ndef imagify_row(row):\n    label = row['label']\n    row = row.drop(['label'])\n    img = Image.fromarray(row.values.reshape(28,28).astype(np.uint8))\n    images.setdefault(label, []).append(img)\n    \n#imagifying rows\ndata.apply(imagify_row , axis = 1)\n\n#picking random image\nlabel = random.choice(list(images.keys()))\nim = random.choice(images[label])\nplt.imshow(im , cmap = \"gray\")\nplt.title(f\"Label: {label}\")\nplt.axis(\"off\")\nplt.show()\n\n#storing images under subfolders\nos.makedirs('images' , exist_ok = True)\nfor label in images.keys():\n    os.makedirs(f\"images/{label}\" , exist_ok = True)\n    for idx, img in enumerate(images[label]):\n        img.save(f\"images/{label}/{label}_{idx}.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T12:56:01.132254Z","iopub.status.idle":"2025-09-24T12:56:01.132571Z","shell.execute_reply.started":"2025-09-24T12:56:01.132434Z","shell.execute_reply":"2025-09-24T12:56:01.132446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Multinomial Logistic Regression","metadata":{}},{"cell_type":"code","source":"class My_LR:\n    def __init__(self , lr=0.01 , n_iters=10000):\n        self.lr = lr\n        self.n_iters = n_iters\n\n    def softmax(self , z):\n        exp_z = np.exp(z - np.max(z , axis = 1 , keepdims = True))\n        return exp_z / np.sum(exp_z , axis = 1 , keepdims = True)\n\n    def fit(self , X , y):\n        n_samples , n_features = X.shape\n        n_classes = len(np.unique(y))\n\n        self.W = np.zeros((n_features , n_classes))\n        self.b = np.zeros((1 , n_classes))\n\n        #OHE y\n        y_ohe = np.eye(n_classes)[y].squeeze()\n\n        #meat\n        for _ in range(self.n_iters):\n            linear = np.dot(X, self.W) + self.b\n            y_pred = self.softmax(linear)\n\n            dW = (1 / n_samples) * np.dot(X.T, (y_pred - y_ohe))\n            db = (1 / n_samples) * np.sum(y_pred - y_ohe, axis=0, keepdims=True)\n\n            self.W -= self.lr * dW\n            self.b -= self.lr * db\n\n    def predict(self , X):\n        linear = np.dot(X, self.W) + self.b\n        y_pred = self.softmax(linear)\n        return np.argmax(y_pred , axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T08:50:55.434750Z","iopub.execute_input":"2025-09-24T08:50:55.435011Z","iopub.status.idle":"2025-09-24T08:50:55.446955Z","shell.execute_reply.started":"2025-09-24T08:50:55.434990Z","shell.execute_reply":"2025-09-24T08:50:55.446160Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nmodel = LogisticRegression(max_iter=2000, multi_class=\"multinomial\" , solver='sag')\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint(\"Accuracy: (API)\", accuracy_score(y_test, preds))\n\n## Turn of my model\nLR = My_LR(0.1 , 2000)\nLR.fit(X_train , y_train)\npreds = LR.predict(X_train)\nprint(\"Accuracy (custom):\", accuracy_score(y_train, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T09:48:20.788922Z","iopub.execute_input":"2025-09-24T09:48:20.789423Z","iopub.status.idle":"2025-09-24T09:54:22.297499Z","shell.execute_reply.started":"2025-09-24T09:48:20.789398Z","shell.execute_reply":"2025-09-24T09:54:22.296721Z"}},"outputs":[{"name":"stdout","text":"Accuracy: (API) 0.9186507936507936\nAccuracy (custom): 0.9169727891156463\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"import joblib\njoblib.dump(model, \"logreg_sklearn.pkl\") # saving sklearn model\nnp.savez(\"logreg_custom.npz\", W=LR.W, b=LR.b) # saving w,b of custom model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T10:19:55.294285Z","iopub.execute_input":"2025-09-24T10:19:55.294590Z","iopub.status.idle":"2025-09-24T10:19:55.301116Z","shell.execute_reply.started":"2025-09-24T10:19:55.294548Z","shell.execute_reply":"2025-09-24T10:19:55.300398Z"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"## Naive Bayes Model","metadata":{}},{"cell_type":"code","source":"class My_NB:\n    def __init__(self , eps=1e-6):\n        self.priors = None\n        self.mean = None\n        self.variances = None\n        self.eps = eps\n    \n    def train(self , X , y):\n        N, D = X.shape\n        classes = np.unique(y)\n        K = len(classes)\n        self.priors = np.zeros(K)\n        self.means = np.zeros((K, D))\n        self.variances = np.zeros((K, D))\n        \n        for idx, k in enumerate(classes):\n            X_k = X[y == k]\n            N_k = X_k.shape[0]\n            self.priors[idx] = N_k / N\n            self.means[idx , :] = np.mean(X_k , axis = 0)\n            self.variances[idx , :] = np.var(X_k , axis = 0) + self.eps\n        self.classes = classes\n        return self.priors , self.mean , self.variances\n\n    def predict(self , X_test):\n        N, D = X_test.shape\n        K = self.priors.shape[0]\n        scores = np.zeros((N , K))\n\n        for k in range(K):\n            prior = np.log(self.priors[k])\n            mu = self.means[k]\n            sigma2 = self.variances[k]\n            log_likelihood = -0.5 * np.sum(np.log(2 * np.pi * sigma2) + ((X_test - mu) ** 2) / (2 * sigma2) , axis=1)\n            scores[:, k] = prior + log_likelihood\n        return self.classes[np.argmax(scores , axis = 1)]\n\n    def evaluate(self , X_test , y_test):\n        y_pred = self.predict(X_test)\n        accuracy = np.sum(y_pred == y_test)\n        return accuracy\n\n    def load_model(self, filepath):\n        data = np.load(filepath, allow_pickle=True)\n        self.priors = data[\"priors\"]\n        self.means = data[\"means\"]\n        self.variances = data[\"variances\"]\n        self.classes = data[\"classes\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:02:37.272710Z","iopub.execute_input":"2025-09-24T13:02:37.273132Z","iopub.status.idle":"2025-09-24T13:02:37.283396Z","shell.execute_reply.started":"2025-09-24T13:02:37.273105Z","shell.execute_reply":"2025-09-24T13:02:37.282357Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nmodel = MultinomialNB()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint(\"Accuracy: (API)\", accuracy_score(y_test, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:02:39.318926Z","iopub.execute_input":"2025-09-24T13:02:39.319261Z","iopub.status.idle":"2025-09-24T13:02:39.427471Z","shell.execute_reply.started":"2025-09-24T13:02:39.319238Z","shell.execute_reply":"2025-09-24T13:02:39.426416Z"}},"outputs":[{"name":"stdout","text":"Accuracy: (API) 0.8256349206349206\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"nb = My_NB(1e-2)\nnb.train(X_train , y_train)\npreds = nb.predict(X_test)\nprint(\"Accuracy: (Custom)\", accuracy_score(y_test, preds))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:04:52.358275Z","iopub.execute_input":"2025-09-24T13:04:52.358583Z","iopub.status.idle":"2025-09-24T13:04:53.591027Z","shell.execute_reply.started":"2025-09-24T13:04:52.358560Z","shell.execute_reply":"2025-09-24T13:04:53.590057Z"}},"outputs":[{"name":"stdout","text":"Accuracy: (Custom) 0.7165079365079365\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"import joblib\njoblib.dump(model, \"naive_bayes_sklearn.pkl\") # saving sklearn model\nnp.savez(\"naive_bayes_custom\",\n        priors=nb.priors,\n        means=nb.means,\n        variances=nb.variances,\n        classes=nb.classes) # saving params of custom model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T13:08:33.434836Z","iopub.execute_input":"2025-09-24T13:08:33.435192Z","iopub.status.idle":"2025-09-24T13:08:33.443139Z","shell.execute_reply.started":"2025-09-24T13:08:33.435145Z","shell.execute_reply":"2025-09-24T13:08:33.442337Z"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":"## Kernelized SVM","metadata":{}},{"cell_type":"markdown","source":"### cupy implementation","metadata":{}},{"cell_type":"code","source":"class My_SVM():\n    def __init__(self, X, y, C=1.0, kernel=\"rbf\", b=0, max_iter=500, tol=1e-5, eps=1e-8):\n        self.X = X\n        self.y = y\n        self.m, self.n = cp.shape(self.X)\n        self.C = float(C)\n        self.error = cp.zeros(self.m)\n        self.max_iter=max_iter\n        self.tol = tol\n        self.eps = eps\n        self.kernel_cache = {}\n        self.alphas = cp.zeros(self.m)\n        self.b = b\n\n        if kernel == \"linear\":\n            self.kernel_func = self.linear_kernel\n            self.is_linear_kernel = True\n        else:\n            self.kernel_func = self.gaussian_kernel\n            self.is_linear_kernel = False\n        \n        self.w = cp.zeros(self.n)  # to be used by linear kernel\n\n    def linear_kernel(self , x1 , x2 , b=0):\n        return x1 @ x2.T + b\n\n    def gaussian_kernel(self , X1 , X2 , sigma=1):\n        X1_sq = cp.sum(X1**2, axis=1)[:, None]\n        X2_sq = cp.sum(X2**2, axis=1)[None, :]\n        dist_sq = X1_sq + X2_sq - 2 * X1 @ X2.T\n        return cp.exp(-dist_sq / (2 * sigma**2))\n            \n    def predict(self, X_test): # update for batch input\n        K_test = self.kernel_func(self.X, X_test)\n        return (self.alphas * self.y) @ K_test + self.b\n\n    def get_error(self, i):\n        return self.predict(self.X[i:i+1, :]) - self.y[i]\n\n    def take_step(self , i1 , i2):\n        if(i1 == i2):\n            return 0\n        x1 = self.X[i1 ,:]\n        x2 = self.X[i2 ,:]\n        y1 = self.y[i1]\n        y2 = self.y[i2]\n        alpha1 = self.alphas[i1]\n        alpha2 = self.alphas[i2]\n        b = self.b\n        E1 = self.get_error(i1)\n        E2 = self.get_error(i2)\n        s = y1 * y2 # sign factor\n\n        if(y1 != y2):\n            L = max(0 , alpha2 - alpha1)\n            H = min(self.C , self.C + alpha2 - alpha1)\n        else:\n            L = max(0, alpha2 + alpha1 - self.C)\n            H = min(self.C, alpha2 + alpha1)\n            \n        if(L == H):\n            return 0 # no room for update\n            \n        K11 = self.K[i1, i1]\n        K12 = self.K[i1, i2]\n        K22 = self.K[i2, i2]\n        eta = K11 + K22 - 2 * K12\n\n        if eta>0: # valid case\n            alpha2_new = alpha2 + y2 * (E2 - E1) / eta\n            if alpha2_new >= H:\n                alpha2_new = H\n            elif alpha2_new <= L:\n                alpha2_new = L \n        else:\n            return 0\n        if abs(alpha2_new - alpha2) < self.eps * (alpha2 + alpha2_new + self.eps):\n            return 0\n\n        alpha1_new = alpha1 + s * (alpha2 - alpha2_new) # $\\alpha_{i1} y_{i1} + \\alpha_{i2} y_{i2} = \\text{constant}$.\n        if alpha1_new < self.eps:\n            alpha1_new = 0\n        elif alpha1_new > (self.C - self.eps):\n            alpha1_new = self.C\n\n        b1 = b - E1 - y1 * (alpha1_new - alpha1) * K11 - y2 * (alpha2_new - alpha2)*K12\n        b2 = b - E2 - y1 * (alpha1_new - alpha1) * K12 - y2 * (alpha2_new - alpha2)*K22\n        if 0 < alpha1_new < self.C:\n            self.b = b1\n        elif 0 < alpha2_new < self.C:\n            self.b = b2\n        else:\n            self.b = 0.5 * (b1 + b2)\n\n        if self.is_linear_kernel:\n            self.w = self.w + y1 * (alpha1_new - alpha1) * x1 + y2 * (alpha2_new - alpha2) * x2\n\n        self.alphas[i1] = alpha1_new\n        self.alphas[i2] = alpha2_new\n        self.error[i1] = 0\n        self.error[i2] = 0\n\n        i_list = [idx for idx, alpha in enumerate(self.alphas) if 0 < alpha and alpha < self.C]\n        self.error = (self.alphas * self.y) @ self.K + self.b - self.y # computing error at once\n        return 1\n\n    def examine_example(self, i2):\n        y2 = self.y[i2]\n        alpha2 = self.alphas[i2]\n        E2 = self.get_error(i2)\n        r2 = E2 * y2\n    \n        if ((r2 < -self.tol and alpha2 < self.C) or (r2 > self.tol and alpha2 > 0)):\n\n            # Non-bound alphas\n            non_bound = cp.where((self.alphas > 0) & (self.alphas < self.C))[0]\n    \n            # Heuristic 1: pick i1 with max |E1 - E2|\n            if non_bound.size > 1:\n                if E2 > 0:\n                    i1 = int(cp.argmin(self.error).item())  # to Python int\n                else:\n                    i1 = int(cp.argmax(self.error).item())\n                if self.take_step(i1, i2):\n                    return 1\n    \n            # Heuristic 2: iterate over non-bound set, with random shift\n            if non_bound.size > 0:\n                shift = cp.random.choice(cp.arange(self.m), size=1).item()\n                i1_list = cp.roll(non_bound, shift)\n                for i1 in i1_list:   # convert indices to CPU list for looping\n                    i1 = int(i1)\n                    if self.take_step(i1, i2):\n                        return 1\n    \n            # Heuristic 3: iterate over full set, with random shift\n            shift = cp.random.choice(cp.arange(self.m), size=1).item()\n            i1_list = cp.roll(cp.arange(self.m), shift)\n            for i1 in i1_list.tolist():       # again, CPU indices are fine here\n                if self.take_step(i1, i2):\n                    return 1\n    \n        return 0\n\n    def fit(self):\n        self.K = self.kernel_func(self.X, self.X)\n        self.error = -self.y.copy()\n        loop_iter = 0\n        num_changed = 0\n        examine_all = True\n        while num_changed > 0 or examine_all:\n            if(loop_iter % 25 == 0):\n                print(\"epoch: \" , loop_iter)\n            if loop_iter >= self.max_iter:\n                break\n            num_changed = 0\n            if examine_all:\n                for i2 in range(self.m):\n                    num_changed += self.examine_example(i2)\n            else:\n                i2_list = [idx for idx, alpha in enumerate(self.alphas) if 0 < alpha and alpha < self.C] # non-bound multipliers\n                for i2 in i2_list:\n                    num_changed += self.examine_example(i2)\n            if examine_all:\n                examine_all = False\n            elif num_changed == 0:\n                examine_all = True\n            loop_iter += 1\n\ndef multiclass_fit(X , y , C=1.0 , kernel = \"rbf\"):\n    K = len(cp.unique(y))\n    models = []\n    for i in range(K):\n        y_k = cp.where(y==i , 1 , -1)\n        svm = My_SVM(X , y_k , C ,  kernel , max_iter=200, tol=1e-5, eps=1e-5)\n        svm.fit()\n        models.append(svm)\n        print(\"Model \" , i , \" trained\")\n    return models\n    \ndef multiclass_predict(X_test , models):\n    scores = cp.zeros((X_test.shape[0] , len(models)))\n    for k , svm in enumerate(models):\n        scores[:, k] = svm.predict(X_test)\n    return cp.argmax(scores , axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T12:48:36.397953Z","iopub.execute_input":"2025-09-26T12:48:36.398241Z","iopub.status.idle":"2025-09-26T12:48:36.418691Z","shell.execute_reply.started":"2025-09-26T12:48:36.398223Z","shell.execute_reply":"2025-09-26T12:48:36.418064Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\n\nX_train_samples = np.array_split(X_train, 10)\nX_test_samples  = np.array_split(X_test, 10)\ny_train_samples = np.array_split(y_train, 10)\ny_test_samples  = np.array_split(y_test, 10)\n\nfor i, (xtr, ytr, x_te, y_te) in enumerate(zip(X_train_samples, y_train_samples, X_test_samples, y_test_samples)):\n    models = multiclass_fit(xtr, ytr, C=1.0, kernel=\"linear\")\n    y_pred_custom = multiclass_predict(x_te, models)\n    accuracy = accuracy_score(y_te, y_pred_custom)\n    print(f\"Chunk {i+1}: accuracy = {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T05:37:37.432740Z","iopub.execute_input":"2025-10-01T05:37:37.433436Z","iopub.status.idle":"2025-10-01T05:37:37.606344Z","shell.execute_reply.started":"2025-10-01T05:37:37.433408Z","shell.execute_reply":"2025-10-01T05:37:37.605137Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/544391018.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my_pred_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulticlass_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_custom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'multiclass_fit' is not defined"],"ename":"NameError","evalue":"name 'multiclass_fit' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"class My_SVM():\n    def __init__(self, X, y, C=1.0, kernel=\"rbf\", b=0, max_iter=500, tol=1e-5, eps=1e-8):\n        self.X = X\n        self.y = y\n        self.m, self.n = np.shape(self.X)\n        self.C = float(C)\n        self.error = np.zeros(self.m)\n        self.max_iter=max_iter\n        self.tol = tol\n        self.eps = eps\n        self.kernel_cache = {}\n        self.alphas = np.zeros(self.m)\n        self.b = b\n\n        if (kernel == 'linear'):\n            self.kernel_func = self.linear_kernel\n        elif kernel == 'gaussian' or kernel == 'rbf':\n            self.kernel_func = self.gaussian_kernel\n        else:\n            raise ValueError('unknown kernel type')\n        self.is_linear_kernel = False\n        if (kernel == 'linear'):\n            self.is_linear_kernel = True if kernel == 'linear' else False\n        \n        self.w = np.zeros(self.n)  # to be used by linear kernel\n\n    def linear_kernel(self , x1 , x2 , b=0):\n        return x1 @ x2.T + b\n\n    def gaussian_kernel(self , x1 , x2 , sigma=1):\n        if np.ndim(x1) == 1 and np.ndim(x2) == 1:\n            return np.exp(-(np.linalg.norm(x1-x2,2))**2/(2*sigma**2))\n        elif(np.ndim(x1)>1 and np.ndim(x2) == 1) or (np.ndim(x1) == 1 and np.ndim(x2)>1):\n            return np.exp(-(np.linalg.norm(x1-x2, 2, axis=1)**2)/(2*sigma**2))\n        elif np.ndim(x1) > 1 and np.ndim(x2) > 1 :\n            return np.exp(-(np.linalg.norm(x1[:, np.newaxis] - x2[np.newaxis, :], 2, axis = 2) ** 2)/(2*sigma**2))\n        return 0.\n            \n    def predict(self, x):\n        result = (self.alphas * self.y) @ self.kernel_func(self.X, x) + self.b\n        return result\n\n    def get_error(self, i):\n        return self.predict(self.X[i,:]) - self.y[i]\n\n    def take_step(self , i1 , i2):\n        if(i1 == i2):\n            return 0\n        x1 = self.X[i1 ,:]\n        x2 = self.X[i2 ,:]\n        y1 = self.y[i1]\n        y2 = self.y[i2]\n        alpha1 = self.alphas[i1]\n        alpha2 = self.alphas[i2]\n        b = self.b\n        E1 = self.get_error(i1)\n        E2 = self.get_error(i2)\n        s = y1 * y2 # sign factor\n\n        if(y1 != y2):\n            L = max(0 , alpha2 - alpha1)\n            H = min(self.C , self.C + alpha2 - alpha1)\n        else:\n            L = max(0, alpha2 + alpha1 - self.C)\n            H = min(self.C, alpha2 + alpha1)\n            \n        if(L == H):\n            return 0 # no room for update\n            \n        K11 = self.kernel_func(x1, x1)\n        K12 = self.kernel_func(x1, x2)\n        K22 = self.kernel_func(x2, x2)\n        eta = K11 + K22 - 2 * K12\n\n        if eta>0: # invalid case\n            alpha2_new = alpha2 + y2 * (E2 - E1) / eta\n            if alpha2_new >= H:\n                alpha2_new = H\n            elif alpha2_new <= L:\n                alpha2_new = L \n        else:\n            return 0\n        if abs(alpha2_new - alpha2) < self.eps * (alpha2 + alpha2_new + self.eps):\n            return 0\n\n        alpha1_new = alpha1 + s * (alpha2 - alpha2_new) # $\\alpha_{i1} y_{i1} + \\alpha_{i2} y_{i2} = \\text{constant}$.\n        if alpha1_new < self.eps:\n            alpha1_new = 0\n        elif alpha1_new > (self.C - self.eps):\n            alpha1_new = self.C\n\n        b1 = b - E1 - y1 * (alpha1_new - alpha1) * K11 - y2 * (alpha2_new - alpha2)*K12\n        b2 = b - E2 - y1 * (alpha1_new - alpha1) * K12 - y2 * (alpha2_new - alpha2)*K22\n        if 0 < alpha1_new < self.C:\n            self.b = b1\n        elif 0 < alpha2_new < self.C:\n            self.b = b2\n        else:\n            self.b = 0.5 * (b1 + b2)\n\n        if self.is_linear_kernel:\n            self.w = self.w + y1 * (alpha1_new - alpha1) * x1 + y2 * (alpha2_new - alpha2) * x2\n\n        self.alphas[i1] = alpha1_new\n        self.alphas[i2] = alpha2_new\n        self.error[i1] = 0\n        self.error[i2] = 0\n\n        i_list = [idx for idx, alpha in enumerate(self.alphas) if 0 < alpha and alpha < self.C]\n        for i in i_list:\n            self.error[i] += y1 * (alpha1_new - alpha1) * self.kernel_func(x1, self.X[i,:]) + y2 * (alpha2_new - alpha2) * self.kernel_func(x2, self.X[i,:]) + (self.b - b)\n        return 1\n\n    def examine_example(self, i2):\n        y2 = self.y[i2]\n        alpha2 = self.alphas[i2]\n        E2 = self.get_error(i2)\n        r2 = E2 * y2\n        if ((r2 < -self.tol and alpha2 < self.C) or (r2 > self.tol and alpha2 > 0)):\n            if len(self.alphas[(0 < self.alphas) & (self.alphas < self.C)]) > 1:\n                if E2 > 0:\n                    i1 = np.argmin(self.error)\n                else:\n                    i1 = np.argmax(self.error)\n                if self.take_step(i1, i2):\n                    return 1\n            i1_list = [idx for idx, alpha in enumerate(self.alphas) if 0 < alpha and alpha < self.C]\n            i1_list = np.roll(i1_list, np.random.choice(np.arange(self.m)))\n            for i1 in i1_list:\n                if self.take_step(i1, i2):\n                    return 1\n            i1_list = np.roll(np.arange(self.m), np.random.choice(np.arange(self.m)) )\n            for i1 in i1_list:\n                if self.take_step(i1, i2):\n                    return 1\n        return 0\n\n    def fit(self):\n        loop_iter = 0\n        num_changed = 0\n        examine_all = True\n        while num_changed > 0 or examine_all:\n            if(loop_iter % 25 == 0):\n                print(\"epoch: \" , loop_iter)\n            if loop_iter >= self.max_iter:\n                break\n            num_changed = 0\n            if examine_all:\n                for i2 in range(self.m):\n                    num_changed += self.examine_example(i2)\n            else:\n                i2_list = [idx for idx, alpha in enumerate(self.alphas) if 0 < alpha and alpha < self.C] # non-bound multipliers\n                for i2 in i2_list:\n                    num_changed += self.examine_example(i2)\n            if examineAll:\n                examineAll = False\n            elif numChanged == 0:\n                examineAll = True\n            loop_iter += 1\n\ndef multiclass_fit(X , y , C=1.0 , kernel = \"rbf\"):\n    K = len(np.unique(y))\n    models = []\n    for i in range(K):\n        y_k = np.where(y==i , 1 , -1)\n        svm = My_SVM(X , y , C ,  kernel , max_iter=600, tol=1e-5, eps=1e-5)\n        svm.fit()\n        models.append(svm)\n        print(\"Model \" , i , \" trained\")\n    return models\n    \ndef multiclass_predict(X_test , models):\n    scores = np.zeros((X_test.shape[0] , len(models)))\n    for k , svm in enumerate(models):\n        scores[: k] = svm.predict(X_test)\n    return np.argmax(scores , axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T05:17:23.117626Z","iopub.execute_input":"2025-09-26T05:17:23.117912Z","iopub.status.idle":"2025-09-26T05:17:23.140487Z","shell.execute_reply.started":"2025-09-26T05:17:23.117882Z","shell.execute_reply":"2025-09-26T05:17:23.139854Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## Breaking data into subsamples","metadata":{}},{"cell_type":"code","source":"def multiclass_fit(X , y , C=1.0 , kernel = \"rbf\"):\n    K = len(np.unique(y))\n    models = []\n    for i in range(K):\n        y_k = np.where(y==i , 1 , -1)\n        svm = SVM(X , y , C ,  kernel , max_iter=600, tol=1e-5, eps=1e-5)\n        svm.fit()\n        models.append(svm)\n        print(\"Model \" , i , \" trained\")\n    return models\n    \ndef multiclass_predict(X_test , models):\n    scores = np.zeros((X_test.shape[0] , len(models)))\n    for k , svm in enumerate(models):\n        scores[: k] = svm.predict(X_test)\n    return np.argmax(scores , axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-26T04:23:34.326548Z","iopub.execute_input":"2025-09-26T04:23:34.326925Z","iopub.status.idle":"2025-09-26T04:23:34.334234Z","shell.execute_reply.started":"2025-09-26T04:23:34.326891Z","shell.execute_reply":"2025-09-26T04:23:34.333047Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from sklearn.svm import SVC\nmodel = SVC(kernel='rbf', decision_function_shape='ovo')\nmodel.fit(X_train , y_train)\ny_pred = model.predict(X_test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-25T19:22:28.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy = accuracy_score(y_test, y_pred_custom)\nprint(f\"Custom accuracy: {accuracy:.2f}\")\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"API accuracy: {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-25T19:22:28.280Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## basic decision tree","metadata":{}},{"cell_type":"code","source":"from collections import namedtuple\n#Node structures\nSplitNode = namedtuple('SplitNode', ['feature', 'threshold', 'left', 'right'])\nLeafNode = namedtuple('LeafNode', ['class_'])\n\nclass My_DT:\n    def __init__(self, max_depth=10, min_samples_split=2):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split # min samples in a split\n        self.tree = None\n\n    def gini(self , y):\n        _ , counts = np.unique(y , return_counts = True)\n        p = counts / len(y)\n        return 1 - np.sum(p**2)\n    \n    def entropy(self , y):\n        pk = np.unique(y , return_counts = True) / len(y)\n        return np.sum(-1.*pk*np.log2(pk + 1e-9))\n\n    def best_split(self , X , y):\n        N, D = X.shape\n        gini_parent = self.gini(y)\n        best_gain = -np.inf\n        best_feature, best_threshold = None, None\n        best_left_X, best_right_X = None, None\n        best_left_y, best_right_y = None, None\n\n        for d in range(D):\n            thresholds = np.unique(X[: , d])\n            for theta in thresholds:\n                left_mask = X[: , d] >= theta\n                right_mask = X[: , d] < theta\n                if np.sum(left_mask) < self.min_samples_split or np.sum(right_mask) < self.min_samples_split:\n                    continue\n                y_left, y_right = y[left_mask], y[right_mask]\n                gini_left = self.gini(y_left)\n                gini_right = self.gini(y_right)\n                gini_split = (len(y_left) / N) * gini_left + (len(y_right) / N) * gini_right\n                gain = gini_parent - gini_split\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature = d\n                    best_threshold = theta\n                    best_left_X, best_right_X = X[left_mask], X[right_mask]\n                    best_left_y, best_right_y = y_left, y_right\n                    \n        return best_feature, best_threshold, best_left_X, best_right_X, best_left_y, best_right_y\n\n    def build_tree(self , X , y , depth = 0):\n        if depth >= self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:\n            return LeafNode(class_ = np.bincount(y.astype(int)).argmax()) # return argmax class\n\n        feature , threshold , left_X , right_X , left_y , right_y = self.best_split(X , y)\n        if feature is None:  # No valid split\n            return LeafNode(class_=np.bincount(y.astype(int)).argmax())\n\n        left_node = self.build_tree(left_X, left_y, depth + 1)\n        right_node = self.build_tree(right_X, right_y, depth + 1)\n        return SplitNode(feature=feature, threshold=threshold, left=left_node, right=right_node)\n\n    def train(self , X , y):\n        self.tree = self.build_tree(X , y)\n\n    def predict_one(self , x , node):\n        if isinstance(node, LeafNode):\n            return node.class_ # if leaf return leaf_class\n        if x[node.feature] <= node.threshold:\n            return self.predict_one(x, node.left) # forward left\n        return self.predict_one(x, node.right) # forward right\n\n    def predict(self , X):\n        return np.array([self.predict_one(x, self.tree) for x in X])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T06:28:32.615080Z","iopub.execute_input":"2025-10-01T06:28:32.615379Z","iopub.status.idle":"2025-10-01T06:28:32.630901Z","shell.execute_reply.started":"2025-10-01T06:28:32.615357Z","shell.execute_reply":"2025-10-01T06:28:32.629811Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dt = My_DT(max_depth=100, min_samples_split=2)\ndt.train(X_train, y_train)\ny_pred = dt.predict(x_test , y_test)\nacc_score = accuracy_score(y_test, y_pred_final)\nprint(f\"Accuracy_: {acc_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T07:23:53.332189Z","iopub.execute_input":"2025-10-01T07:23:53.332483Z","execution_failed":"2025-10-01T11:18:25.947Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random_Forest","metadata":{}},{"cell_type":"code","source":"from collections import namedtuple\n#Node structures\nSplitNode = namedtuple('SplitNode', ['feature', 'threshold', 'left', 'right'])\nLeafNode = namedtuple('LeafNode', ['class_'])\n\nclass My_DT_rf:\n    def __init__(self, max_depth=10, min_samples_split=2 , max_features = None):\n        self.max_depth = max_depth\n        self.min_samples_split = min_samples_split # min samples in a split\n        self.max_features = max_features\n        self.tree = None\n\n    def gini(self , y):\n        _ , counts = np.unique(y , return_counts = True)\n        p = counts / len(y)\n        return 1 - np.sum(p**2)\n    \n    def entropy(self , y):\n        pk = np.unique(y , return_counts = True) / len(y)\n        return np.sum(-1.*pk*np.log2(pk + 1e-9))\n\n    def best_split(self , X , y):\n        N, D = X.shape\n        features = np.arange(D)\n        if self.max_features is not None:\n            features = np.random.choice(D, self.max_features, replace=False)\n        gini_parent = self.gini(y)\n        best_gain = -np.inf\n        best_feature, best_threshold = None, None\n        best_left_idx, best_right_idx = None, None\n\n        for d in features:\n            thresholds = np.unique(X[:, d])\n            for thr in thresholds:\n                left_mask = X[:, d] <= thr\n                right_mask = ~left_mask\n                if np.sum(left_mask) < self.min_samples_split or np.sum(right_mask) < self.min_samples_split:\n                    continue\n                gini_left = self.gini(y[left_mask])\n                gini_right = self.gini(y[right_mask])\n                gini_split = (np.sum(left_mask)/N) * gini_left + (np.sum(right_mask)/N) * gini_right\n                gain = self.gini(y) - gini_split\n                if gain > best_gain:\n                    best_gain = gain\n                    best_feature, best_threshold = d, thr\n                    best_left_idx, best_right_idx = left_mask, right_mask\n\n        return best_feature, best_threshold, best_left_idx, best_right_idx\n\n    def build_tree(self , X , y , depth = 0):\n        if depth >= self.max_depth or len(y) < self.min_samples_split or len(np.unique(y)) == 1:\n            return LeafNode(class_ = np.bincount(y.astype(int)).argmax()) # return argmax class\n\n        feature , threshold , left_idx , right_idx = self.best_split(X , y)\n        if feature is None:  # No valid split\n            return LeafNode(class_=np.bincount(y.astype(int)).argmax())\n\n        left_node = self.build_tree(X[left_idx], y[left_idx], depth + 1)\n        right_node = self.build_tree(X[right_idx], y[right_idx], depth + 1)\n        return SplitNode(feature=feature, threshold=threshold, left=left_node, right=right_node)\n\n    def train(self , X , y):\n        self.tree = self.build_tree(X , y)\n\n    def predict_one(self , x , node):\n        if isinstance(node, LeafNode):\n            return node.class_ # if leaf return leaf_class\n        if x[node.feature] <= node.threshold:\n            return self.predict_one(x, node.left) # forward left\n        return self.predict_one(x, node.right) # forward right\n\n    def predict(self , X):\n        return np.array([self.predict_one(x, self.tree) for x in X])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T11:30:20.461124Z","iopub.execute_input":"2025-10-01T11:30:20.461776Z","iopub.status.idle":"2025-10-01T11:30:20.486505Z","shell.execute_reply.started":"2025-10-01T11:30:20.461732Z","shell.execute_reply":"2025-10-01T11:30:20.484778Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pickle\nfrom sklearn.metrics import accuracy_score\ndef bootstrap_sample(X , y):\n    n_samples = X.shape[0]\n    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n    return X[indices], y[indices]\n\nn_trees = 50\nn_features = int(np.sqrt(X_train.shape[1]))\nmodels=[]\nfor i in range(n_trees):\n    X_boot , y_boot = bootstrap_sample(X_train , y_train)\n    dt = My_DT_rf(max_depth=20, min_samples_split=2 , max_features = n_features)\n    dt.train(X_boot, y_boot)\n    models.append(dt)\n    print(\"Tree\" , i , \"trained\")\nall_preds = []\nfor model in models:\n    preds = model.predict(X_test) #full test set predict\n    all_preds.append(preds)\n\nall_preds = np.array(all_preds)\ny_pred_final = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=all_preds) #lambda majority voting\nacc_score = accuracy_score(y_test, y_pred_final)\nprint(f\"RF Accuracy: {acc_score:.4f}\")\n\n#saving tree\nwith open(\"forest.pkl\", \"wb\") as f:\n    pickle.dump([tree.__dict__ for tree in models], f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T15:48:26.771889Z","iopub.execute_input":"2025-10-01T15:48:26.773927Z","iopub.status.idle":"2025-10-01T17:45:56.047871Z","shell.execute_reply.started":"2025-10-01T15:48:26.773860Z","shell.execute_reply":"2025-10-01T17:45:56.046670Z"}},"outputs":[{"name":"stdout","text":"Tree 0 trained\nTree 1 trained\nTree 2 trained\nTree 3 trained\nTree 4 trained\nTree 5 trained\nTree 6 trained\nTree 7 trained\nTree 8 trained\nTree 9 trained\nTree 10 trained\nTree 11 trained\nTree 12 trained\nTree 13 trained\nTree 14 trained\nTree 15 trained\nTree 16 trained\nTree 17 trained\nTree 18 trained\nTree 19 trained\nTree 20 trained\nTree 21 trained\nTree 22 trained\nTree 23 trained\nTree 24 trained\nTree 25 trained\nTree 26 trained\nTree 27 trained\nTree 28 trained\nTree 29 trained\nTree 30 trained\nTree 31 trained\nTree 32 trained\nTree 33 trained\nTree 34 trained\nTree 35 trained\nTree 36 trained\nTree 37 trained\nTree 38 trained\nTree 39 trained\nTree 40 trained\nTree 41 trained\nTree 42 trained\nTree 43 trained\nTree 44 trained\nTree 45 trained\nTree 46 trained\nTree 47 trained\nTree 48 trained\nTree 49 trained\nRF Accuracy_: 0.9578\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### sklearn random_forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nimport joblib\nsklearn_rf = RandomForestClassifier(n_estimators = 50)\nsklearn_rf.fit(X_train , y_train)\ny_pred = sklearn_rf.predict(X_test)\nacc_score = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy_API: {acc_score:.4f}\")\nfilename = 'random_forest_sklearn.pkl'\njoblib.dump(sklearn_rf, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T18:20:20.815056Z","iopub.execute_input":"2025-10-01T18:20:20.815447Z","iopub.status.idle":"2025-10-01T18:20:32.799191Z","shell.execute_reply.started":"2025-10-01T18:20:20.815414Z","shell.execute_reply":"2025-10-01T18:20:32.798287Z"}},"outputs":[{"name":"stdout","text":"Accuracy_API: 0.9575\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['random_forest_sklearn.pkl']"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"# Gradient Boost Implementation","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.tree import DecisionTreeRegressor\nclass GB_classifier:\n    def __init__(self , learning_rate = 0.1 , n_trees = 100 , max_depth = 3):\n        self.learning_rate = learning_rate\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        \n    def _softmax(self , preds):\n        exp = np.exp(preds)\n        return exp / np.sum(exp , axis = 1 , keepdims = True)\n\n    def train(self , X , y):\n        self.K = len(np.unique(y))\n        self.trees = {k: [] for k in range(self.K)} # multiclass forest of trees\n        ohe_y = np.eye(self.K)[y.astype(int)]\n        preds = np.zeros(ohe_y.shape)\n\n        for i in range(self.n_trees):\n            probs = self._softmax(preds)\n            \n            for k in range(self.K):\n                # k class logitboost\n                numerator = (ohe_y.T[k] - probs.T[k])\n                denominator = probs.T[k] * (1 - probs.T[k])\n                residuals = (self.K - 1) / self.K * numerator / denominator\n                weights = denominator\n\n                tree = DecisionTreeRegressor(criterion='friedman_mse', max_depth=self.max_depth)\n                tree.fit(X , residuals , sample_weight = weights)\n                self.trees[k].append(tree)\n                leaf_index = tree.apply(X) # returns leaf index\n                preds.T[k] += self.learning_rate * tree.predict(X)\n            \n    def predict(self , x):\n        preds = np.zeros((len(x) , self.K))\n        for i in range(self.n_trees):\n            for k in range(self.K):\n                preds.T[k] += self.learning_rate * self.trees[k][i].predict(x)\n        return np.argmax(preds , axis = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:41:42.782563Z","iopub.execute_input":"2025-10-03T15:41:42.782917Z","iopub.status.idle":"2025-10-03T15:41:42.794516Z","shell.execute_reply.started":"2025-10-03T15:41:42.782893Z","shell.execute_reply":"2025-10-03T15:41:42.793625Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\ngb_dt = GB_classifier()\ngb_dt.train(X_train, y_train)\ngb_dt_preds = gb_dt.predict(X_test)\ngbc_accuracy = accuracy_score(y_test, gb_dt_preds)\nprint(f'gb_dt accuracy: {gbc_accuracy}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T15:41:42.983106Z","iopub.execute_input":"2025-10-03T15:41:42.983682Z","iopub.status.idle":"2025-10-03T16:12:00.332377Z","shell.execute_reply.started":"2025-10-03T15:41:42.983653Z","shell.execute_reply":"2025-10-03T16:12:00.331248Z"}},"outputs":[{"name":"stdout","text":"gb_dt accuracy: 0.9465079365079365\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/986596862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgbc_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgb_dt_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'gb_dt accuracy: {gbc_accuracy}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbc_pred_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'gbc_pred_res' is not defined"],"ename":"NameError","evalue":"name 'gbc_pred_res' is not defined","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"import pickle\nwith open(\"gbdt_model.pkl\", \"wb\") as f:\n    pickle.dump(gb_dt, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T16:13:52.398209Z","iopub.execute_input":"2025-10-03T16:13:52.399156Z","iopub.status.idle":"2025-10-03T16:13:52.429296Z","shell.execute_reply.started":"2025-10-03T16:13:52.399129Z","shell.execute_reply":"2025-10-03T16:13:52.428636Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## sklearn implementation","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)\ngb_classifier.fit(X_train, y_train)\ny_pred = gb_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"sklearn_gbdt acc:- {accuracy:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T17:22:35.027843Z","iopub.execute_input":"2025-10-03T17:22:35.028081Z","iopub.status.idle":"2025-10-03T17:50:04.135718Z","shell.execute_reply.started":"2025-10-03T17:22:35.028057Z","shell.execute_reply":"2025-10-03T17:50:04.134972Z"}},"outputs":[{"name":"stdout","text":"sklearn_gbdt acc:- 0.94\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class My_KNN: # classifier\n    def __init__(self , k = 5):\n        self.k = k\n        \n    def get_data(self , X , y):\n        self.X_train = X\n        self.y_train = y\n        \n    def _euclidian_distance(self , x_test):\n        return np.linalg.norm(self.X_train - x_test , axis = 1) # root((a-b)^2) euclidian distance with all \n\n    def _find_neighbour(self , x_test):\n        distances = self._euclidian_distance(x_test)\n        k_nearest = np.argsort(distances)[:self.k] # select first k closest indexes\n        targets = self.y_train[k_nearest]\n        return np.bincount(targets.astype(int)).argmax()\n\n    def predict(self , X_test):\n        return np.array([self._find_neighbour(x) for x in X_test])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T16:59:23.123020Z","iopub.execute_input":"2025-10-03T16:59:23.123316Z","iopub.status.idle":"2025-10-03T16:59:23.129262Z","shell.execute_reply.started":"2025-10-03T16:59:23.123295Z","shell.execute_reply":"2025-10-03T16:59:23.128375Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nmy_knn = My_KNN()\nmy_knn.get_data(X_train , y_train)\ny_preds = my_knn.predict(X_test)\nacc_score = accuracy_score(y_test , y_preds)\nprint(\"Knn accuracy:- \" , acc_score)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T16:59:23.247197Z","iopub.execute_input":"2025-10-03T16:59:23.247472Z","iopub.status.idle":"2025-10-03T17:22:34.720602Z","shell.execute_reply.started":"2025-10-03T16:59:23.247453Z","shell.execute_reply":"2025-10-03T17:22:34.719795Z"}},"outputs":[{"name":"stdout","text":"Knn accuracy:-  0.9655555555555555\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"with open(\"my_knn.pkl\", \"wb\") as f:\n    pickle.dump(my_knn, f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T17:22:34.721894Z","iopub.execute_input":"2025-10-03T17:22:34.722127Z","iopub.status.idle":"2025-10-03T17:22:35.027039Z","shell.execute_reply.started":"2025-10-03T17:22:34.722110Z","shell.execute_reply":"2025-10-03T17:22:35.026015Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"import joblib\nfilename = 'gbdt_sklearn.pkl'\njoblib.dump(gb_classifier, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T17:52:16.894296Z","iopub.execute_input":"2025-10-03T17:52:16.894629Z","iopub.status.idle":"2025-10-03T17:52:16.928772Z","shell.execute_reply.started":"2025-10-03T17:52:16.894600Z","shell.execute_reply":"2025-10-03T17:52:16.928016Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['gbdt_sklearn.pkl']"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"### sklearn implementation","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors=5)\nknn_classifier.fit(X_train, y_train)\ny_pred = knn_classifier.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"sklearn KNN acc: {accuracy}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T17:51:50.537358Z","iopub.execute_input":"2025-10-03T17:51:50.537688Z","iopub.status.idle":"2025-10-03T17:51:59.997932Z","shell.execute_reply.started":"2025-10-03T17:51:50.537663Z","shell.execute_reply":"2025-10-03T17:51:59.997093Z"}},"outputs":[{"name":"stdout","text":"sklearn KNN acc: 0.9655555555555555\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"filename = 'knn_sklearn.pkl'\njoblib.dump(knn_classifier, filename)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-03T17:53:04.167247Z","iopub.execute_input":"2025-10-03T17:53:04.167562Z","iopub.status.idle":"2025-10-03T17:53:04.361496Z","shell.execute_reply.started":"2025-10-03T17:53:04.167538Z","shell.execute_reply":"2025-10-03T17:53:04.360704Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"['knn_sklearn.pkl']"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}